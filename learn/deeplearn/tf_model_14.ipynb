{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\u001b[1m2518/2518\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 35ms/step - accuracy: 0.9114 - auc: 0.9678 - f1_score: 0.6671 - loss: 0.3474 - precision: 0.9072 - recall: 0.9168 - val_accuracy: 0.9308 - val_auc: 0.9728 - val_f1_score: 0.3086 - val_loss: 0.2707 - val_precision: 0.7722 - val_recall: 0.8804 - learning_rate: 9.7551e-04\n",
      "Epoch 2/300\n",
      "\u001b[1m2518/2518\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 33ms/step - accuracy: 0.9283 - auc: 0.9777 - f1_score: 0.6669 - loss: 0.2728 - precision: 0.9233 - recall: 0.9343 - val_accuracy: 0.9296 - val_auc: 0.9721 - val_f1_score: 0.3086 - val_loss: 0.2471 - val_precision: 0.7696 - val_recall: 0.8768 - learning_rate: 9.0444e-04\n",
      "Epoch 3/300\n",
      "\u001b[1m2518/2518\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 34ms/step - accuracy: 0.9288 - auc: 0.9783 - f1_score: 0.6691 - loss: 0.2497 - precision: 0.9234 - recall: 0.9360 - val_accuracy: 0.9250 - val_auc: 0.9733 - val_f1_score: 0.3086 - val_loss: 0.2403 - val_precision: 0.7430 - val_recall: 0.9002 - learning_rate: 7.9374e-04\n",
      "Epoch 4/300\n",
      "\u001b[1m2518/2518\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 33ms/step - accuracy: 0.9330 - auc: 0.9805 - f1_score: 0.6666 - loss: 0.2296 - precision: 0.9279 - recall: 0.9390 - val_accuracy: 0.9289 - val_auc: 0.9726 - val_f1_score: 0.3086 - val_loss: 0.2250 - val_precision: 0.7654 - val_recall: 0.8796 - learning_rate: 6.5427e-04\n",
      "Epoch 5/300\n",
      "\u001b[1m2518/2518\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 34ms/step - accuracy: 0.9347 - auc: 0.9818 - f1_score: 0.6665 - loss: 0.2183 - precision: 0.9298 - recall: 0.9405 - val_accuracy: 0.9277 - val_auc: 0.9739 - val_f1_score: 0.3086 - val_loss: 0.2189 - val_precision: 0.7577 - val_recall: 0.8877 - learning_rate: 4.9969e-04\n",
      "Epoch 6/300\n",
      "\u001b[1m2518/2518\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 34ms/step - accuracy: 0.9361 - auc: 0.9822 - f1_score: 0.6663 - loss: 0.2111 - precision: 0.9305 - recall: 0.9425 - val_accuracy: 0.9314 - val_auc: 0.9739 - val_f1_score: 0.3086 - val_loss: 0.2112 - val_precision: 0.7717 - val_recall: 0.8859 - learning_rate: 3.4514e-04\n",
      "Epoch 7/300\n",
      "\u001b[1m2518/2518\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 36ms/step - accuracy: 0.9369 - auc: 0.9828 - f1_score: 0.6666 - loss: 0.2056 - precision: 0.9316 - recall: 0.9432 - val_accuracy: 0.9354 - val_auc: 0.9740 - val_f1_score: 0.3086 - val_loss: 0.2009 - val_precision: 0.7960 - val_recall: 0.8682 - learning_rate: 2.0575e-04\n",
      "Epoch 8/300\n",
      "\u001b[1m2518/2518\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 47ms/step - accuracy: 0.9387 - auc: 0.9835 - f1_score: 0.6666 - loss: 0.1992 - precision: 0.9346 - recall: 0.9434 - val_accuracy: 0.9299 - val_auc: 0.9744 - val_f1_score: 0.3086 - val_loss: 0.2055 - val_precision: 0.7660 - val_recall: 0.8867 - learning_rate: 9.5199e-05\n",
      "Epoch 9/300\n",
      "\u001b[1m2518/2518\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 55ms/step - accuracy: 0.9405 - auc: 0.9852 - f1_score: 0.6661 - loss: 0.1892 - precision: 0.9356 - recall: 0.9461 - val_accuracy: 0.9333 - val_auc: 0.9746 - val_f1_score: 0.3086 - val_loss: 0.1991 - val_precision: 0.7820 - val_recall: 0.8794 - learning_rate: 2.4299e-05\n",
      "Epoch 10/300\n",
      "\u001b[1m2518/2518\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 54ms/step - accuracy: 0.9408 - auc: 0.9847 - f1_score: 0.6676 - loss: 0.1903 - precision: 0.9359 - recall: 0.9467 - val_accuracy: 0.9210 - val_auc: 0.9695 - val_f1_score: 0.3086 - val_loss: 0.2332 - val_precision: 0.7348 - val_recall: 0.8872 - learning_rate: 1.0000e-03\n",
      "Epoch 11/300\n",
      "\u001b[1m2517/2518\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9331 - auc: 0.9805 - f1_score: 0.6677 - loss: 0.2161 - precision: 0.9276 - recall: 0.9399"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "This optimizer was created with a `LearningRateSchedule` object as its `learning_rate` constructor argument, hence its learning rate is not settable. If you need the learning rate to be settable, you should instantiate the optimizer with a float `learning_rate` argument.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 120\u001b[0m\n\u001b[1;32m    110\u001b[0m reduce_lr \u001b[38;5;241m=\u001b[39m ReduceLROnPlateau(\n\u001b[1;32m    111\u001b[0m     monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_f1_score\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    112\u001b[0m     factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    116\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    117\u001b[0m )\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_balanced\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_lr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m    128\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[1;32m    131\u001b[0m test_results \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test_scaled, y_test, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:597\u001b[0m, in \u001b[0;36mBaseOptimizer.learning_rate\u001b[0;34m(self, learning_rate)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    594\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    595\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_learning_rate, learning_rate_schedule\u001b[38;5;241m.\u001b[39mLearningRateSchedule\n\u001b[1;32m    596\u001b[0m     ):\n\u001b[0;32m--> 597\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    598\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis optimizer was created with a `LearningRateSchedule`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    599\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m object as its `learning_rate` constructor argument, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    600\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhence its learning rate is not settable. If you need the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    601\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m learning rate to be settable, you should instantiate \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    602\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe optimizer with a float `learning_rate` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    603\u001b[0m         )\n\u001b[1;32m    604\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_learning_rate\u001b[38;5;241m.\u001b[39massign(learning_rate)\n\u001b[1;32m    605\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prev_lr_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    606\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_learning_rate, backend\u001b[38;5;241m.\u001b[39mVariable\n\u001b[1;32m    607\u001b[0m ):\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;66;03m# Untrack learning rate variable\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: This optimizer was created with a `LearningRateSchedule` object as its `learning_rate` constructor argument, hence its learning rate is not settable. If you need the learning rate to be settable, you should instantiate the optimizer with a float `learning_rate` argument."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras import regularizers\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras.metrics import AUC, Precision, Recall, F1Score\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('../../data/preprocessed_data.csv')\n",
    "X = data.drop('Depression', axis=1)\n",
    "y = data['Depression']\n",
    "\n",
    "# Handle NaN values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "\n",
    "# Split data\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Apply SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_balanced)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Calculate class weights\n",
    "class_counts = np.bincount(y_train_balanced.astype(int))\n",
    "total_samples = len(y_train_balanced)\n",
    "class_weights = {\n",
    "    0: total_samples / (2 * class_counts[0]),\n",
    "    1: total_samples / (2 * class_counts[1])\n",
    "}\n",
    "\n",
    "# Attention layer function\n",
    "def attention_layer(inputs):\n",
    "    attention_probs = layers.Dense(inputs.shape[-1], activation='softmax')(inputs)\n",
    "    return layers.Multiply()([inputs, attention_probs])\n",
    "\n",
    "# Model architecture\n",
    "inputs = layers.Input(shape=(X_train.shape[1],))\n",
    "x = layers.GaussianNoise(0.05)(inputs)\n",
    "\n",
    "# First block with matching dimensions for residual connection\n",
    "residual = layers.Dense(2048, kernel_initializer='he_uniform')(x)  # Projection for residual\n",
    "x = layers.Dense(2048, kernel_initializer='he_uniform')(x)\n",
    "x = layers.LeakyReLU(negative_slope=0.1)(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = attention_layer(x)\n",
    "x = layers.Dropout(0.4)(x)\n",
    "x = layers.Add()([residual, x])  # Now dimensions match\n",
    "\n",
    "# Dense blocks\n",
    "for units in [1024, 512, 256]:\n",
    "    x = layers.Dense(units, kernel_initializer='he_uniform',\n",
    "                    kernel_regularizer=regularizers.l1_l2(l1=1e-6, l2=1e-5))(x)\n",
    "    x = layers.LeakyReLU(negative_slope=0.1)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "\n",
    "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Learning rate schedule\n",
    "initial_learning_rate = 0.001\n",
    "decay_steps = len(X_train_scaled) // 64 * 10  # Update steps per epoch\n",
    "lr_schedule = keras.optimizers.schedules.CosineDecayRestarts(\n",
    "    initial_learning_rate, first_decay_steps=decay_steps, alpha=1e-6\n",
    ")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = keras.optimizers.AdamW(\n",
    "    learning_rate=lr_schedule,\n",
    "    weight_decay=1e-4,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    amsgrad=True\n",
    ")\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy',\n",
    "             AUC(name='auc'),\n",
    "             Precision(name='precision'),\n",
    "             Recall(name='recall'),\n",
    "             F1Score(name='f1_score')]\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_f1_score',  # Focus on F1 Score for imbalanced data\n",
    "    patience=30,\n",
    "    restore_best_weights=True,\n",
    "    mode='max'\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_f1_score',\n",
    "    factor=0.2,\n",
    "    patience=10,\n",
    "    min_lr=1e-7,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train_balanced,\n",
    "    validation_data=(X_val_scaled, y_val),\n",
    "    epochs=300,\n",
    "    batch_size=64,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "test_results = model.evaluate(X_test_scaled, y_test, verbose=1)\n",
    "print(\"\\nTest Results:\")\n",
    "print(f\"Accuracy: {test_results[1]:.4f}\")\n",
    "print(f\"AUC: {test_results[2]:.4f}\")\n",
    "print(f\"Precision: {test_results[3]:.4f}\")\n",
    "print(f\"Recall: {test_results[4]:.4f}\")\n",
    "print(f\"F1 Score: {test_results[5]:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
